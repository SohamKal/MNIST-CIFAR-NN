{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrO7HrS6Sux-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TODO: Finish and submit your code for logistic regression, neural network, and hyperparameter search.\n",
        "\n",
        "\"\"\"\n",
        "import random\n",
        "import itertools\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from collections import namedtuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(28 * 28, 10)  # 28x28 image input, 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        return self.linear(x)  # Forward pass\n",
        "\n",
        "\n",
        "\"\"\" - Part 1 - \"\"\"\n",
        "\n",
        "\n",
        "def logistic_regression(device):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "    train_dataset = datasets.MNIST(\n",
        "        root='./MNIST_dataset', train=True, download=True, transform=transform)\n",
        "\n",
        "    train_size = 48000\n",
        "    val_size = 12000\n",
        "    train_set, _ = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "\n",
        "    model = LogisticRegressionModel().to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                          lr=0.011, weight_decay=0.001)\n",
        "\n",
        "    epochs = 15\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    results = dict(\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\"\"\" - Part 2 - \"\"\"\n",
        "\n",
        "\n",
        "class FNN(nn.Module):\n",
        "    def __init__(self, loss_type, num_classes):\n",
        "        super(FNN, self).__init__()\n",
        "\n",
        "        self.loss_type = loss_type\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, num_classes)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        output = self.fc3(x)\n",
        "        if self.loss_type != \"ce\":\n",
        "            output = F.softmax(output, dim=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_loss(self, output, target):\n",
        "        if self.loss_type in ['cross_entropy', 'ce']:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(output, target)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "\"\"\" - Part 3 - \"\"\"\n",
        "\n",
        "\n",
        "def tune_hyper_parameter(target_metric, device):\n",
        "    # Define the search space for hyperparameters (reduced for faster tuning)\n",
        "    learning_rates = [0.001,0.00075,0.00001]\n",
        "    weight_decays = [0.0001,0.00001]\n",
        "    batch_sizes = [64,128]\n",
        "\n",
        "    # Named tuple to store hyperparameters\n",
        "    HyperParams = namedtuple(\n",
        "        'HyperParams', ['learning_rate', 'batch_size', 'weight_decay'])\n",
        "\n",
        "    # Initialize variables to track the best results\n",
        "    best_logistic_params = None\n",
        "    best_fnn_params = None\n",
        "    best_logistic_metric = float('-inf')\n",
        "    best_fnn_metric = float('-inf')\n",
        "\n",
        "    # Define the dataloader creation logic inside the function\n",
        "    def create_dataloaders(batch_size, dataset_name=\"MNIST\"):\n",
        "        if dataset_name == \"MNIST\":\n",
        "            transform = transforms.Compose(\n",
        "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "            train_dataset = datasets.MNIST(\n",
        "                root='./MNIST_dataset', train=True, download=True, transform=transform)\n",
        "            test_dataset = datasets.MNIST(\n",
        "                root='./MNIST_dataset', train=False, download=True, transform=transform)\n",
        "            # Using smaller training size to speed up tuning\n",
        "            train_size = 50000\n",
        "            val_size = 10000\n",
        "            train_set, val_set = random_split(\n",
        "                train_dataset, [train_size, val_size])\n",
        "            train_loader = DataLoader(\n",
        "                train_set, batch_size=batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(\n",
        "                val_set, batch_size=batch_size, shuffle=False)\n",
        "            test_loader = DataLoader(\n",
        "                test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        elif dataset_name == \"CIFAR10\":\n",
        "            transform = transforms.Compose(\n",
        "                [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "            train_dataset = datasets.CIFAR10(\n",
        "                root='./CIFAR10_dataset', train=True, download=True, transform=transform)\n",
        "            test_dataset = datasets.CIFAR10(\n",
        "                root='./CIFAR10_dataset', train=False, download=True, transform=transform)\n",
        "            # Using smaller training size to speed up tuning\n",
        "            train_size = 45000\n",
        "            val_size = 5000\n",
        "            train_set, val_set = random_split(\n",
        "                train_dataset, [train_size, val_size])\n",
        "            train_loader = DataLoader(\n",
        "                train_set, batch_size=batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(\n",
        "                val_set, batch_size=batch_size, shuffle=False)\n",
        "            test_loader = DataLoader(\n",
        "                test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "    # --- Training function ---\n",
        "    def train(model, optimizer, train_loader, device, epochs=5, patience=2):\n",
        "        model.train()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        best_val_acc = -float('inf')\n",
        "        no_improve_epochs = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            model.train()\n",
        "            pbar = tqdm(train_loader, ncols=100, position=0, leave=True)\n",
        "            for batch_idx, (data, target) in enumerate(pbar):\n",
        "                optimizer.zero_grad()\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "\n",
        "                if hasattr(model, 'get_loss'):\n",
        "                    loss = model.get_loss(output, target)\n",
        "                else:\n",
        "                    loss = criterion(output, target)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "            # Early stopping logic\n",
        "            val_accuracy = validation(model, val_loader, device)\n",
        "            if val_accuracy > best_val_acc:\n",
        "                best_val_acc = val_accuracy\n",
        "                no_improve_epochs = 0\n",
        "            else:\n",
        "                no_improve_epochs += 1\n",
        "                if no_improve_epochs >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "                    break\n",
        "\n",
        "    # --- Validation function ---\n",
        "    def validation(model, validation_loader, device):\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in validation_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "        return 100 * correct / total\n",
        "\n",
        "    # --- Logistic Regression Hyperparameter Search (MNIST) ---\n",
        "    for lr, wd, batch_size in itertools.product(learning_rates, weight_decays, batch_sizes):\n",
        "        print(f\"Testing Logistic Regression with lr: {lr}, weight_decay: {wd}, batch_size: {batch_size}\")\n",
        "        train_loader, val_loader, _ = create_dataloaders(\n",
        "            batch_size=batch_size, dataset_name=\"MNIST\")\n",
        "        model = LogisticRegressionModel().to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "        train(model, optimizer, train_loader,\n",
        "              device, epochs=8)  # Train the model\n",
        "        val_accuracy = validation(\n",
        "            model, val_loader, device)  # Validate the model\n",
        "        if val_accuracy > best_logistic_metric:\n",
        "            best_logistic_metric = val_accuracy\n",
        "            best_logistic_params = HyperParams(lr, batch_size, wd)\n",
        "\n",
        "    # --- FNN Hyperparameter Search (CIFAR-10) ---\n",
        "    for lr, wd, batch_size in itertools.product(learning_rates, weight_decays, batch_sizes):\n",
        "        print(f\"Testing FNN with lr: {lr}, weight_decay: {wd}, batch_size: {batch_size}\")\n",
        "        train_loader, val_loader, _ = create_dataloaders(\n",
        "            batch_size=batch_size, dataset_name=\"CIFAR10\")\n",
        "        model = FNN(loss_type='cross_entropy', num_classes=10).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "        train(model, optimizer, train_loader,\n",
        "              device, epochs=8)  # Train the model\n",
        "        val_accuracy = validation(\n",
        "            model, val_loader, device)  # Validate the model\n",
        "        if val_accuracy > best_fnn_metric:\n",
        "            best_fnn_metric = val_accuracy\n",
        "            best_fnn_params = HyperParams(lr, batch_size, wd)\n",
        "\n",
        "    # Return the best parameters and metrics for both models\n",
        "    best_params = [\n",
        "        {\n",
        "            \"logistic_regression\": {\n",
        "                \"learning_rate\": best_logistic_params.learning_rate,\n",
        "                \"batch_size\": best_logistic_params.batch_size,\n",
        "                \"weight_decay\": best_logistic_params.weight_decay\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"FNN\": {\n",
        "                \"learning_rate\": best_fnn_params.learning_rate,\n",
        "                \"batch_size\": best_fnn_params.batch_size,\n",
        "                \"weight_decay\": best_fnn_params.weight_decay\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    best_metric = [\n",
        "        {\n",
        "            \"logistic_regression\": {\n",
        "                \"accuracy\": best_logistic_metric\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"FNN\": {\n",
        "                \"accuracy\": best_fnn_metric\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    return best_params, best_metric\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import timeit\n",
        "\n",
        "\n",
        "\n",
        "class Params:\n",
        "    class BatchSize:\n",
        "        train = 128\n",
        "        val = 128\n",
        "        test = 1000\n",
        "\n",
        "    def __init__(self):\n",
        "        self.mode = 'tune'\n",
        "        # self.model = 'tune'\n",
        "        self.target_metric = 'accuracy'\n",
        "        # self.target_metric = 'loss'\n",
        "\n",
        "        self.device = 'gpu'\n",
        "        self.loss_type = \"ce\"\n",
        "        self.batch_size = Params.BatchSize()\n",
        "        self.n_epochs = 10\n",
        "        self.learning_rate = 1e-1\n",
        "        self.momentum = 0.5\n",
        "\n",
        "\n",
        "def get_dataloaders(batch_size):\n",
        "\n",
        "    import torch\n",
        "    from torch.utils.data import random_split\n",
        "    import torchvision\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    :param Params.BatchSize batch_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    CIFAR_training = torchvision.datasets.CIFAR10('.', train=True, download=True,\n",
        "                                                  transform=torchvision.transforms.Compose([\n",
        "                                                      torchvision.transforms.ToTensor(),\n",
        "                                                      torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
        "\n",
        "    CIFAR_test_set = torchvision.datasets.CIFAR10('.', train=False, download=True,\n",
        "                                                  transform=torchvision.transforms.Compose([\n",
        "                                                      torchvision.transforms.ToTensor(),\n",
        "                                                      torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
        "\n",
        "    # create a training and a validation set\n",
        "    CIFAR_train_set, CIFAR_val_set = random_split(\n",
        "        CIFAR_training, [40000, 10000])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        CIFAR_train_set, batch_size=batch_size.train, shuffle=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        CIFAR_val_set, batch_size=batch_size.val, shuffle=False)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(CIFAR_test_set,\n",
        "                                              batch_size=batch_size.test, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def train(net, optimizer, train_loader, device):\n",
        "    net.train()\n",
        "    pbar = tqdm(train_loader, ncols=100, position=0, leave=True)\n",
        "    avg_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        optimizer.zero_grad()\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = net(data)\n",
        "        loss = net.get_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_sc = loss.item()\n",
        "\n",
        "        avg_loss += (loss_sc - avg_loss) / (batch_idx + 1)\n",
        "\n",
        "        pbar.set_description(\n",
        "            'train loss: {:.6f} avg loss: {:.6f}'.format(loss_sc, avg_loss))\n",
        "\n",
        "\n",
        "def validation(net, validation_loader, device):\n",
        "    net.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in validation_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = net(data)\n",
        "        loss = net.get_loss(output, target)\n",
        "        validation_loss += loss.item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    validation_loss /= len(validation_loader.dataset)\n",
        "    print('\\nValidation set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        validation_loss, correct, len(validation_loader.dataset),\n",
        "        100. * correct / len(validation_loader.dataset)))\n",
        "\n",
        "\n",
        "def test(net, test_loader, device):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = net(data)\n",
        "        loss = net.get_loss(output, target)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "def main():\n",
        "    params = Params()\n",
        "\n",
        "    try:\n",
        "        import paramparse\n",
        "    except ImportError:\n",
        "        print(\"paramparse is unavailable so commandline arguments will not work\")\n",
        "    else:\n",
        "        paramparse.process(params)\n",
        "\n",
        "    import torch\n",
        "    import torch.optim as optim\n",
        "\n",
        "    import torch.nn.functional as F\n",
        "    import torchvision\n",
        "\n",
        "    random_seed = 1\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "    if params.device != 'cpu' and torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print('Running on GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print('Running on CPU')\n",
        "\n",
        "    if params.mode == 'fnn':\n",
        "        train_loader, val_loader, test_loader = get_dataloaders(\n",
        "            params.batch_size)\n",
        "\n",
        "        net = FNN(params.loss_type, 10).to(device)\n",
        "        optimizer = optim.SGD(net.parameters(), lr=params.learning_rate,\n",
        "                              momentum=params.momentum)\n",
        "\n",
        "        start = timeit.default_timer()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            validation(net, val_loader, device)\n",
        "        for epoch in range(params.n_epochs):\n",
        "            print(f'\\nepoch {epoch + 1} / {params.n_epochs}\\n')\n",
        "            train_start = timeit.default_timer()\n",
        "\n",
        "            train(net, optimizer, train_loader, device)\n",
        "\n",
        "            train_stop = timeit.default_timer()\n",
        "            train_runtime = train_stop - train_start\n",
        "            print(f'\\ntrain runtime: {train_runtime:.2f} secs')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                validation(net, val_loader, device)\n",
        "        with torch.no_grad():\n",
        "            test(net, test_loader, device)\n",
        "\n",
        "        stop = timeit.default_timer()\n",
        "\n",
        "        runtime = stop - start\n",
        "\n",
        "        print(f'total runtime: {runtime:.2f} secs')\n",
        "\n",
        "    elif params.mode == 'tune':\n",
        "        start = timeit.default_timer()\n",
        "        best_params, best_metric = tune_hyper_parameter(\n",
        "            params.target_metric, device)\n",
        "        stop = timeit.default_timer()\n",
        "        run_time = stop - start\n",
        "        print()\n",
        "        print(f\"Best {params.target_metric}: {best_metric}\")\n",
        "        print(f\"Best params:\\n{best_params}\")\n",
        "        print(f\"runtime of tune_hyper_parameter: {run_time}\")\n",
        "    else:\n",
        "        raise AssertionError(f'invalid mode: {params.mode}')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d4E__JUaLa4",
        "outputId": "84ccd6c7-3008-4bfd-927c-7b9c06fa4fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paramparse is unavailable so commandline arguments will not work\n",
            "Running on GPU: Tesla T4\n",
            "Testing Logistic Regression with lr: 0.001, weight_decay: 0.0001, batch_size: 64\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST_dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16103076.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST_dataset/MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST_dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST_dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 488669.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST_dataset/MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST_dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST_dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4524664.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST_dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST_dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST_dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 12211877.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST_dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST_dataset/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:17<00:00, 45.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 0.4090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 65.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 0.2983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:12<00:00, 62.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 0.2832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 65.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 0.2774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 0.2715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/8], Loss: 0.2686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/8], Loss: 0.2652\n",
            "Early stopping triggered after 7 epochs\n",
            "Testing Logistic Regression with lr: 0.001, weight_decay: 0.0001, batch_size: 128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 0.4655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 0.3061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 0.2876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:10<00:00, 36.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 0.2781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 0.2722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/8], Loss: 0.2678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/8], Loss: 0.2640\n",
            "Early stopping triggered after 7 epochs\n",
            "Testing Logistic Regression with lr: 0.001, weight_decay: 1e-05, batch_size: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 0.4084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 0.2952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 0.2809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 0.2741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 68.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 0.2690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/8], Loss: 0.2653\n",
            "Early stopping triggered after 6 epochs\n",
            "Testing Logistic Regression with lr: 0.001, weight_decay: 1e-05, batch_size: 128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 0.4630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 0.3064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 0.2868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 0.2778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 0.2728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/8], Loss: 0.2669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/8], Loss: 0.2639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:10<00:00, 35.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/8], Loss: 0.2609\n",
            "Early stopping triggered after 8 epochs\n",
            "Testing Logistic Regression with lr: 0.00075, weight_decay: 0.0001, batch_size: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 0.4356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:12<00:00, 62.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 0.2970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 0.2820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 0.2727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 67.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 0.2668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/8], Loss: 0.2629\n",
            "Early stopping triggered after 6 epochs\n",
            "Testing Logistic Regression with lr: 0.00075, weight_decay: 0.0001, batch_size: 128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 0.5017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 0.3150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 0.2907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 0.2803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 0.2739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/8], Loss: 0.2679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/8], Loss: 0.2639\n",
            "Early stopping triggered after 7 epochs\n",
            "Testing Logistic Regression with lr: 0.00075, weight_decay: 1e-05, batch_size: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 67.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 0.4293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 67.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 0.2983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 0.2837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 0.2753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:12<00:00, 62.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 0.2706\n",
            "Early stopping triggered after 5 epochs\n",
            "Testing Logistic Regression with lr: 0.00075, weight_decay: 1e-05, batch_size: 128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 0.5117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 0.3148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 0.2908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 0.2800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 0.2735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:10<00:00, 36.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/8], Loss: 0.2694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/8], Loss: 0.2653\n",
            "Early stopping triggered after 7 epochs\n",
            "Testing Logistic Regression with lr: 1e-05, weight_decay: 0.0001, batch_size: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 1.9650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 1.3715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 67.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 1.0516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 0.8642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 0.7429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 67.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/8], Loss: 0.6591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/8], Loss: 0.5979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/8], Loss: 0.5520\n",
            "Testing Logistic Regression with lr: 1e-05, weight_decay: 0.0001, batch_size: 128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 2.2277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 1.7824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 1.4609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 1.2320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 1.0676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:10<00:00, 35.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/8], Loss: 0.9463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:10<00:00, 36.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/8], Loss: 0.8538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:10<00:00, 36.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/8], Loss: 0.7813\n",
            "Testing Logistic Regression with lr: 1e-05, weight_decay: 1e-05, batch_size: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 2.0092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 67.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 1.4111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 1.0795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 67.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 0.8828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 67.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 0.7560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/8], Loss: 0.6684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 68.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/8], Loss: 0.6051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 782/782 [00:11<00:00, 66.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/8], Loss: 0.5571\n",
            "Testing Logistic Regression with lr: 1e-05, weight_decay: 1e-05, batch_size: 128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 2.1501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 1.7286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 1.4340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 1.2223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 1.0676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/8], Loss: 0.9508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 34.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/8], Loss: 0.8605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 391/391 [00:11<00:00, 35.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/8], Loss: 0.7889\n",
            "Testing FNN with lr: 0.001, weight_decay: 0.0001, batch_size: 64\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./CIFAR10_dataset/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43332921.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./CIFAR10_dataset/cifar-10-python.tar.gz to ./CIFAR10_dataset\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 704/704 [00:12<00:00, 54.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 2.1221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 704/704 [00:12<00:00, 55.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/8], Loss: 2.0707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 704/704 [00:12<00:00, 55.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/8], Loss: 2.0524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 704/704 [00:12<00:00, 55.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/8], Loss: 2.0404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 704/704 [00:12<00:00, 55.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/8], Loss: 2.0342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 704/704 [00:12<00:00, 55.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/8], Loss: 2.0263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 704/704 [00:12<00:00, 55.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/8], Loss: 2.0202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 704/704 [00:12<00:00, 55.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/8], Loss: 2.0173\n",
            "Testing FNN with lr: 0.001, weight_decay: 0.0001, batch_size: 128\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████████████████████| 352/352 [00:11<00:00, 29.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/8], Loss: 2.1451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|██████████████████████████▎                                  | 152/352 [00:04<00:06, 31.23it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "#from A1_submission import logistic_regression\n",
        "\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "\n",
        "def compute_score(acc, acc_thresh):\n",
        "    min_thres, max_thres = acc_thresh\n",
        "    if acc <= min_thres:\n",
        "        score = 0.0\n",
        "    elif acc >= max_thres:\n",
        "        score = 100.0\n",
        "    else:\n",
        "        score = float(acc - min_thres) / (max_thres - min_thres) \\\n",
        "                     * 100\n",
        "    return score\n",
        "\n",
        "\n",
        "def test(\n",
        "        model,\n",
        "        device,\n",
        "\n",
        "):\n",
        "    test_dataset = datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(data)\n",
        "            predicted = torch.argmax(output, dim=1)\n",
        "            total += targets.size(0)\n",
        "            num_correct += (predicted == targets).sum().item()\n",
        "\n",
        "    acc = float(num_correct) / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "class Args:\n",
        "    \"\"\"\n",
        "    command-line arguments\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    'logistic': run logistic regression on the specified dataset (part 1)\n",
        "    'tune': run hyper parameter tuning (part 3)\n",
        "    \"\"\"\n",
        "    mode = 'logistic'\n",
        "\n",
        "    \"\"\"\n",
        "    metric with respect to which hyper parameters are to be tuned\n",
        "    'acc': validation classification accuracy\n",
        "    'loss': validation loss\n",
        "    \"\"\"\n",
        "    target_metric = 'acc'\n",
        "    # target_metric = 'loss'\n",
        "\n",
        "    \"\"\"\n",
        "    set to 0 to run on cpu\n",
        "    \"\"\"\n",
        "    gpu = 1\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = Args()\n",
        "    try:\n",
        "        import paramparse\n",
        "        paramparse.process(args)\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    device = torch.device(\"cuda\" if args.gpu and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    acc_thresh = dict(\n",
        "        logistic=(0.83, 0.93),\n",
        "    )\n",
        "\n",
        "    if args.mode == 'logistic':\n",
        "        start = timeit.default_timer()\n",
        "        results = logistic_regression(device)\n",
        "        model = results['model']\n",
        "\n",
        "        if model is None:\n",
        "            print('model is None')\n",
        "            return\n",
        "\n",
        "        stop = timeit.default_timer()\n",
        "        run_time = stop - start\n",
        "\n",
        "        accuracy = test(\n",
        "            model,\n",
        "            device,\n",
        "        )\n",
        "\n",
        "        score = compute_score(accuracy, acc_thresh[args.mode])\n",
        "        result = OrderedDict(\n",
        "            accuracy=accuracy,\n",
        "            score=score,\n",
        "            run_time=run_time\n",
        "        )\n",
        "        print(f\"result on {args.mode}:\")\n",
        "        for key in result:\n",
        "            print(f\"\\t{key}: {result[key]}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL8Mr_llaOME",
        "outputId": "a4366b7f-d277-4128-d9ca-4bd9e74e994b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15], Loss: 0.4864\n",
            "Epoch [2/15], Loss: 0.3381\n",
            "Epoch [3/15], Loss: 0.3149\n",
            "Epoch [4/15], Loss: 0.3024\n",
            "Epoch [5/15], Loss: 0.2941\n",
            "Epoch [6/15], Loss: 0.2887\n",
            "Epoch [7/15], Loss: 0.2843\n",
            "Epoch [8/15], Loss: 0.2807\n",
            "Epoch [9/15], Loss: 0.2778\n",
            "Epoch [10/15], Loss: 0.2750\n",
            "Epoch [11/15], Loss: 0.2728\n",
            "Epoch [12/15], Loss: 0.2711\n",
            "Epoch [13/15], Loss: 0.2691\n",
            "Epoch [14/15], Loss: 0.2677\n",
            "Epoch [15/15], Loss: 0.2662\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 47851260.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1970102.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 14309165.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3677708.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result on logistic:\n",
            "\taccuracy: 0.9201\n",
            "\tscore: 90.1\n",
            "\trun_time: 206.18473782100045\n"
          ]
        }
      ]
    }
  ]
}